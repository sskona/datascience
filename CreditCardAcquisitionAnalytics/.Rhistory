No.of.trades.opened.in.last.12.months,
data = train_data [, -1],
method = "glm",
family="binomial",
preProcess = c("scale", "center"),
tuneLength = 5,
trControl = trainControl(method = "cv",
number = 5,
verboseIter = TRUE,
sampling = "down"))
test_pred_fullCustomerData_glm_undersampling <- predict(model_glm_fullCustomerData_undersampling,
type = "prob",
newdata = test_data)
t9 <- Sys.time()
t9-t8
# Time difference of 2.016878 secs
summary(test_pred_fullCustomerData_glm_undersampling[,2])
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
# 0.2164  0.2935  0.4670  0.4590  0.6035  0.8538
cutOff <- findOptimalCutOff(test_pred_fullCustomerData_glm_undersampling[,2], 0.20, 0.85)
# Optimal Cutoff =  0.541
model_glm_fullCustomerData_undersampling_metrics <- evaluateClassificationModel(test_pred_fullCustomerData_glm_undersampling[,2],
test_actual_default,
cutOff)
rownames(model_glm_fullCustomerData_undersampling_metrics) <- "FullData        - GLM - Under-Sampling"
model_Metrics <- rbind(model_Metrics, model_glm_fullCustomerData_undersampling_metrics)
# Accuracy    : 0.6345
# Sensitivity : 0.62288
# Specificity : 0.06988
#
# F: 0.04046375
#
# Area under the curve (AUC): 0.6289243
# -------------------------------- Logistic Regression - Using Over Sampling
t8 <- Sys.time()
model_glm_fullCustomerData_oversampling <- caret::train(Performance ~
Income_imputed +
No.of.months.in.current.company +
No.of.months.in.current.residence +
Avgas.CC.Utilization.in.last.12.months_WoE +
Avgas.CC.Utilization.in.last.12.months_imputed +
Outstanding.Balance_WoE +
Outstanding.Balance_imputed +
No.of.times.30.DPD.or.worse.in.last.6.months +
No.of.trades.opened.in.last.12.months,
data = train_data [, -1],
method = "glm",
family="binomial",
preProcess = c("scale", "center"),
tuneLength = 5,
trControl = trainControl(method = "cv",
number = 5,
verboseIter = TRUE,
sampling = "up"))
test_pred_fullCustomerData_glm_oversampling <- predict(model_glm_fullCustomerData_oversampling,
type = "prob",
newdata = test_data)
t9 <- Sys.time()
t9-t8
# Time difference of 7.105892 secs
summary(test_pred_fullCustomerData_glm_oversampling[,2])
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
# 0.2224  0.3040  0.4597  0.4587  0.5897  0.8765
cutOff <- findOptimalCutOff(test_pred_fullCustomerData_glm_oversampling[,2], 0.2, 0.87)
# Optimal Cutoff =  0.532
model_glm_fullCustomerData_oversampling_metrics <- evaluateClassificationModel(test_pred_fullCustomerData_glm_oversampling[,2],
test_actual_default,
cutOff)
rownames(model_glm_fullCustomerData_oversampling_metrics) <- "FullData        - GLM - Over-Sampling"
model_Metrics <- rbind(model_Metrics, model_glm_fullCustomerData_oversampling_metrics)
# Accuracy    : 0.6264865
# Sensitivity : 0.6375991
# Specificity : 0.6259972
#
# F: 0.04046375
#
# Area under the curve (AUC): 0.6317982
#
# -------------------------------- Logistic Regression - Using SMOTE
t8 <- Sys.time()
model_glm_fullCustomerData_smote <- caret::train(Performance ~
Income_imputed +
No.of.months.in.current.company +
No.of.months.in.current.residence +
Avgas.CC.Utilization.in.last.12.months_WoE +
Avgas.CC.Utilization.in.last.12.months_imputed +
Outstanding.Balance_WoE +
Outstanding.Balance_imputed +
No.of.times.30.DPD.or.worse.in.last.6.months +
No.of.trades.opened.in.last.12.months,
data = train_data [, -1],
method = "glm",
family="binomial",
preProcess = c("scale", "center"),
tuneLength = 5,
trControl = trainControl(method = "cv",
number = 5,
verboseIter = TRUE,
sampling = "smote"))
test_pred_fullCustomerData_glm_smote <- predict(model_glm_fullCustomerData_smote,
type = "prob",
newdata = test_data)
t9 <- Sys.time()
t9-t8
# Time difference of 10.60025 secs
summary(test_pred_fullCustomerData_glm_smote[,2])
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
# 0.1617  0.2449  0.3912  0.3948  0.5260  0.8312
cutOff <- findOptimalCutOff(test_pred_fullCustomerData_glm_smote[,2], 0.16, 0.83)
# Optimal Cutoff =  0.458
model_glm_fullCustomerData_smotesampling_metrics <- evaluateClassificationModel(test_pred_fullCustomerData_glm_smote[,2],
test_actual_default,
cutOff)
rownames(model_glm_fullCustomerData_smotesampling_metrics) <- "FullData        - GLM - SMOTE-Sampling"
model_Metrics <- rbind(model_Metrics, model_glm_fullCustomerData_smotesampling_metrics)
# Accuracy    : 0.6270118
# Sensitivity : 0.6398641
# Specificity : 0.626446
#
# F: 0.04046375
#
# Area under the curve (AUC): 0.6326885
# -------------------------------- Decision Tree - Using Under-Sampling
#           Accuracy Sensitivity Specificity    F_score Threshold       AUC False_positive_Rate True_positive_Rate
# Accuracy 0.05372749   0.9852775   0.0127144 0.04046375      0.05 0.5010041           0.9852775          0.9872856
# Discarding this Under-sampling options for Decision Tree
# -------------------------------- Decision Tree - Using Over-Sampling
#           Accuracy Sensitivity Specificity    F_score Threshold       AUC False_positive_Rate True_positive_Rate
# Accuracy 0.8581594   0.1313703   0.8901576 0.04046375      0.05 0.5107639           0.1098424          0.1313703
# Discarding this over-sampling options for Decision Tree
# -------------------------------- Decision Tree - Using SMOTE
t8 <- Sys.time()
model_rpart_fullCustomerData_smote <- caret::train(Performance ~
Income_imputed +
No.of.months.in.current.company +
No.of.months.in.current.residence +
#Avgas.CC.Utilization.in.last.12.months_WoE +
Avgas.CC.Utilization.in.last.12.months_imputed +
#Outstanding.Balance_WoE +
Outstanding.Balance_imputed +
No.of.times.30.DPD.or.worse.in.last.6.months +
No.of.trades.opened.in.last.12.months,
data = train_data [, -1],
method = "rpart",
#preProcess = c("scale", "center"),
#minsplit=30, minbucket = 15, cp=0.0001,
tuneLength = 5,
trControl = trainControl(method = "cv",
number = 5,
verboseIter = TRUE,
sampling = "smote"))
# Fitting cp = 0.000388 on full training set
t9 <- Sys.time()
t9-t8
# Time difference of 16.57429 secs
# # plot(model_rpart_fullCustomerData_smote)
# # prp(model_rpart_fullCustomerData_smote, box.palette = "Reds", tweak = 1.2)
# library(rpart.plot)
# rpart.plot(model_rpart_fullCustomerData_smote$finalModel)
test_pred_fullCustomerData_rpart_smote <- predict(model_rpart_fullCustomerData_smote,
type = "prob",
newdata = test_data)
summary(test_pred_fullCustomerData_rpart_smote[,2])
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
# 0.0000  0.1083  0.1765  0.2533  0.2500  1.0000
cutOff <- findOptimalCutOff(test_pred_fullCustomerData_rpart_smote[,2], 0.1, 0.25)
# Optimal Cutoff =  0.206
model_rpart_fullCustomerData_smotesampling_metrics <- evaluateClassificationModel(test_pred_fullCustomerData_rpart_smote[,2],
test_actual_default,
cutOff)
rownames(model_rpart_fullCustomerData_smotesampling_metrics) <- "FullData        - RPART - SMOTE-Sampling"
model_Metrics <- rbind(model_Metrics, model_rpart_fullCustomerData_smotesampling_metrics)
# Accuracy    : 0.6015569
# Sensitivity : 0.599094
# Specificity : 0.6016653
#
# F: 0.04046375
#
# Area under the curve (AUC): 0.6003797
# -------------------------------- Random Forest - Using Under-Sampling
# Remove code snippet related to this model
#           Accuracy Sensitivity Specificity    F_score Threshold       AUC False_positive_Rate True_positive_Rate
# Accuracy 0.6067147   0.6070215   0.6067012 0.04046375     0.522 0.6068614           0.3932988          0.6070215
# Discarding this Under-sampling options for RF
# -------------------------------- Random Forest - Using Over-Sampling
# Remove code snippet related to this model
#           Accuracy Sensitivity Specificity    F_score Threshold       AUC False_positive_Rate True_positive_Rate
# Accuracy 0.5677444   0.5934315   0.5666135 0.04046375      0.05 0.5800225           0.4333865          0.5934315
# Also takes 50+ mins to build model. So, discarding RF Over-Sampling option
# Discarding this Over-sampling options for RF
# -------------------------------- Random Forest - Using SMOTE
t12 <- Sys.time()
model_rf_fullCustomerData_DemographicData_smote <- caret::train(Performance ~
Income_imputed +
No.of.months.in.current.company +
No.of.months.in.current.residence +
Avgas.CC.Utilization.in.last.12.months_WoE +
Avgas.CC.Utilization.in.last.12.months_imputed +
Outstanding.Balance_WoE +
Outstanding.Balance_imputed +
No.of.times.30.DPD.or.worse.in.last.6.months +
No.of.trades.opened.in.last.12.months,
data = train_data [, -1],
method = "rf",
ntree = 1000,
preProcess = c("scale", "center"),
trControl = trainControl(method = "cv",
number = 5,
verboseIter = TRUE,
sampling = "smote"))
t13 <- Sys.time()
t13-t12
# Time difference of 4.712344 mins
test_pred_fullCustomerData_rf_smote <- predict(model_rf_fullCustomerData_DemographicData_smote,
type = "prob",
newdata = test_data)
summary(test_pred_fullCustomerData_rf_smote[,2])
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
# 0.0010  0.0980  0.2240  0.2435  0.3650  0.8620
cutOff <- findOptimalCutOff(test_pred_fullCustomerData_rf_smote[,2], .001,.86)
# Optimal Cutoff =  0.279
model_rf_fullCustomerData_smotesampling_metrics <- evaluateClassificationModel(test_pred_fullCustomerData_rf_smote[,2],
test_actual_default,
cutOff)
rownames(model_rf_fullCustomerData_smotesampling_metrics) <- "FullData        - RF - SMOTE-Sampling"
model_Metrics <- rbind(model_Metrics, model_rf_fullCustomerData_smotesampling_metrics)
plot(varImp(object=model_rf_fullCustomerData_DemographicData_smote),main="Random Forest (SMOTE) - Variable Importance")
# Accuracy    : 0.6212
# Sensitivity : 0.62288
# Specificity : 0.62111
#
# F: 0.040
#
# Area under the curve (AUC): 0.622
#---------------------------------------------------------- Evaluation of Models for Final Model Selection ---------------------------------------------------------------------
# Evaluate various metrics across vall models built
# Analyze Accuracy, Sensitivity, Specificity, AUC, F-Score
model_Metrics
# Evaluating based on AUC, F-Score, Sensitivity, Specificity and Accuracy
#                                              Accuracy Sensitivity Specificity    F_score Threshold       AUC False_positive_Rate True_positive_Rate
# DemographicData - GLM   - Unbalanced        0.5392330   0.5537939   0.5385919 0.04046375     0.042 0.5461929           0.4614081          0.5537939
# FullData        - GLM   - Unbalanced        0.6333636   0.6172140   0.6340746 0.04046375     0.049 0.6256443           0.3659254          0.6172140
# FullData        - GLM   - Under-Sampling    0.6344620   0.6228766   0.6349721 0.04046375     0.541 0.6289243           0.3650279          0.6228766
# FullData        - GLM   - Over-Sampling     0.6223793   0.6387316   0.6216594 0.04046375     0.532 0.6301955           0.3783406          0.6387316
# FullData        - GLM   - SMOTE-Sampling    0.6271551   0.6387316   0.6266454 0.04046375     0.458 0.6326885           0.3733546          0.6387316
# FullData        - RF    - SMOTE-Sampling    0.6055208   0.6092865   0.6053550 0.04046375     0.279 0.6073208           0.3946450          0.6092865
# FullData        - RPART - SMOTE-Sampling    0.6110607   0.5922990   0.6118867 0.04046375     0.203 0.6020928           0.3881133          0.5922990
#
# Analyse Lift, Gain and KS-Statistic metrics
model_Metrics$KSStatistic [1] <- GainLiftChart_KSStatistic(logistic_model_demographic_data_unbalanced, test_data,  "response")
model_Metrics$Lift [1] <- 1.1
model_Metrics$Gain [1] <- 59.91
# KS-Statistic = 0.1065518
# A tibble: 10 x 6
# bucket total totalresp Cumresp      Gain  Cumlift
# <int> <int>     <dbl>   <dbl>     <dbl>    <dbl>
# 1      1  2094       131     131  14.83579 1.483579
# 2      2  2094       103     234  26.50057 1.325028
# 3      3  2094       105     339  38.39185 1.279728
# 4      4  2094        88     427  48.35787 1.208947
# 5      5  2094       102     529  59.90940 1.198188
# 6      6  2094        70     599  67.83692 1.130615
# 7      7  2094        88     687  77.80294 1.111471
# 8      8  2094        69     756  85.61721 1.070215
# 9      9  2094        59     815  92.29898 1.025544
# 10     10  2093        68     883 100.00000 1.000000
model_Metrics$KSStatistic [2] <- GainLiftChart_KSStatistic(logistic_model_application_and_creditdata_unbalanced, test_data,  "response")
model_Metrics$Lift [2] <- 1.49
model_Metrics$Gain [2] <- 74.97
# KS-Statistic = 0.2666422
# A tibble: 10 x 6
# bucket total totalresp Cumresp      Gain  Cumlift
# <int> <int>     <dbl>   <dbl>     <dbl>    <dbl>
# 1      1  2094       181     181  20.49830 2.049830
# 2      2  2094       149     330  37.37259 1.868630
# 3      3  2094       135     465  52.66138 1.755379
# 4      4  2094       109     574  65.00566 1.625142
# 5      5  2094        88     662  74.97169 1.499434
# 6      6  2094        73     735  83.23896 1.387316
# 7      7  2094        48     783  88.67497 1.266785
# 8      8  2094        50     833  94.33749 1.179219
# 9      9  2094        32     865  97.96149 1.088461
# 10     10  2093        18     883 100.00000 1.000000
model_Metrics$KSStatistic [3] <- GainLiftChart_KSStatistic(model_glm_fullCustomerData_undersampling, test_data,  "raw")
model_Metrics$Lift [3] <- 1.43
model_Metrics$Gain [3] <- 71.80
# KS-Statistic = 0.2532234
# A tibble: 10 x 6
# bucket total totalresp Cumresp      Gain  Cumlift
# <int> <int>     <int>   <int>     <dbl>    <dbl>
#   1      1  2094       124     124  14.04304 1.404304
# 2      2  2094       129     253  28.65232 1.432616
# 3      3  2094       148     401  45.41336 1.513779
# 4      4  2094       138     539  61.04190 1.526048
# 5      5  2094        95     634  71.80068 1.436014
# 6      6  2094        47     681  77.12344 1.285391
# 7      7  2094        50     731  82.78596 1.182657
# 8      8  2094        48     779  88.22197 1.102775
# 9      9  2094        50     829  93.88448 1.043161
# 10     10  2093        54     883 100.00000 1.000000
model_Metrics$KSStatistic [4] <- GainLiftChart_KSStatistic(model_glm_fullCustomerData_oversampling, test_data,  "raw")
model_Metrics$Lift [4] <- 1.44
model_Metrics$Gain [4] <- 72.23
# KS-Statistic = 0.2568917
# # A tibble: 10 x 6
# bucket total totalresp Cumresp      Gain  Cumlift
# <int> <int>     <int>   <int>     <dbl>    <dbl>
#   1      1  2094       125     125  14.15629 1.415629
# 2      2  2094       133     258  29.21857 1.460929
# 3      3  2094       148     406  45.97961 1.532654
# 4      4  2094       140     546  61.83465 1.545866
# 5      5  2094        90     636  72.02718 1.440544
# 6      6  2094        46     682  77.23669 1.287278
# 7      7  2094        48     730  82.67271 1.181039
# 8      8  2094        48     778  88.10872 1.101359
# 9      9  2094        49     827  93.65798 1.040644
# 10     10  2093        56     883 100.00000 1.000000
model_Metrics$KSStatistic [5] <-GainLiftChart_KSStatistic(model_glm_fullCustomerData_smote, test_data, "raw")
model_Metrics$Lift [5] <- 1.27
model_Metrics$Gain [5] <- 63.75
# KS-Statistic = 0.2153007
# A tibble: 10 x 6
# bucket total totalresp Cumresp      Gain  Cumlift
# <int> <int>     <int>   <int>     <dbl>    <dbl>
#   1      1  2094       132     132  14.94904 1.494904
# 2      2  2094       163     295  33.40883 1.670442
# 3      3  2094       153     448  50.73613 1.691204
# 4      4  2094        56     504  57.07814 1.426954
# 5      5  2094        59     563  63.75991 1.275198
# 6      6  2094        54     617  69.87542 1.164590
# 7      7  2094        69     686  77.68969 1.109853
# 8      8  2094        63     749  84.82446 1.060306
# 9      9  2094        69     818  92.63873 1.029319
# 10     10  2093        65     883 100.00000 1.000000
model_Metrics$KSStatistic [6] <-GainLiftChart_KSStatistic(model_rpart_fullCustomerData_smote, test_data, "raw")
model_Metrics$Lift [6] <- 1.08
model_Metrics$Gain [6] <- 54.36
# KS-Statistic = 0.1127819
# # A tibble: 10 x 6
# bucket total totalresp Cumresp      Gain  Cumlift
# <int> <int>     <int>   <int>     <dbl>    <dbl>
#   1      1  2094       149     149  16.87429 1.687429
# 2      2  2094       108     257  29.10532 1.455266
# 3      3  2094        72     329  37.25934 1.241978
# 4      4  2094        78     407  46.09287 1.152322
# 5      5  2094        73     480  54.36014 1.087203
# 6      6  2094        88     568  64.32616 1.072103
# 7      7  2094        73     641  72.59343 1.037049
# 8      8  2094        78     719  81.42695 1.017837
# 9      9  2094        77     796  90.14723 1.001636
# 10     10  2093        87     883 100.00000 1.000000
model_Metrics$KSStatistic [7] <- GainLiftChart_KSStatistic(model_rf_fullCustomerData_DemographicData_smote, test_data, "raw")
model_Metrics$Lift [7] <- 1.03
model_Metrics$Gain [7] <- 51.64
# KS-Statistic = 0.0643504
0.0643504
# A tibble: 10 x 6
# bucket total totalresp Cumresp      Gain   Cumlift
# <int> <int>     <int>   <int>     <dbl>     <dbl>
#   1      1  2094       140     140  15.85504 1.5855040
# 2      2  2094        66     206  23.32956 1.1664779
# 3      3  2094        86     292  33.06908 1.1023028
# 4      4  2094        71     363  41.10985 1.0277463
# 5      5  2094        93     456  51.64213 1.0328426
# 6      6  2094        83     539  61.04190 1.0173650
# 7      7  2094        87     626  70.89468 1.0127811
# 8      8  2094        83     709  80.29445 1.0036806
# 9      9  2094        83     792  89.69422 0.9966025
# 10     10  2093        91     883 100.00000 1.0000000
# [1] 0.06415095
#------------------------------------------------------------------------ Final Model Selection ------------------------------------------------------------------------
# Top 2 Models Selected
# Note :
# -----
#        1) Discarding Random Forest as it involves high computational resources,
#           and also not providing any better formance
#        2) Discarding GLM/Unbalanced with Full Data, as well because it is trained with unbalanced data
# FullData        - GLM   - SMOTE-Sampling
# FullData        - GLM   - Over-Sampling
#
# Discarding GLM/Unbalanced model though it has highest KS-Statistic value = 0.2666422 as it is based on Unabalanced data
# GLM/Over-Sampling model has better KS-Statistic value = 0.2568917 than GLM/SMOTE model KS-Statistic value = 0.2153007
#final_Model_For_Scorecard <- model_glm_fullCustomerData_oversampling
#final_Model_For_Scorecard$finalModel
final_Model_For_Scorecard <- logistic_model_application_and_creditdata_unbalanced
# Generalized Linear Model
#
# 48863 samples
# 9 predictor
# 2 classes: '0', '1'
#
# Pre-processing: scaled (9), centered (9)
# Resampling: Cross-Validated (5 fold)
# Summary of sample sizes: 39090, 39091, 39091, 39090, 39090
# Addtional sampling using up-sampling prior to pre-processing
#
# Resampling results:
#
#   Accuracy   Kappa
# 0.5782698  0.04827151
# Coefficients:
#   (Intercept)                                  Income_imputed
# -0.00420                                        -0.01460
# No.of.months.in.current.company               No.of.months.in.current.residence
# -0.03083                                        -0.06005
# Avgas.CC.Utilization.in.last.12.months_WoE  Avgas.CC.Utilization.in.last.12.months_imputed
# -0.27858                                         0.11162
# Outstanding.Balance_WoE                     Outstanding.Balance_imputed
# -0.12984                                        -0.02812
# No.of.times.30.DPD.or.worse.in.last.6.months           No.of.trades.opened.in.last.12.months
# 0.22748                                         0.14213
#
# Degrees of Freedom: 93599 Total (i.e. Null);  93590 Residual
# Null Deviance:	    129800
# Residual Deviance: 120700 	AIC: 120700
#---------------------------------------------------------- Application Scorecard Building  ---------------------------------------------------------------------
App_Scorecard <- function(model,testdataset){
m <- model
score_data <- testdataset
score_data$bad <- predict(m,type="response",newdata = score_data[,-12])
score_data$good <- (1- score_data$bad)
score_data$odds <- score_data$good/score_data$bad
score_data$logodds <- log(score_data$odds)
points0 = 400
odds0 = 10
pdo = 20
factor = pdo / log(2)
offset = points0 - factor * log( odds0 )
score_data$Score <- offset + factor * score_data$logodds
return(score_data)
}
testdata_scorecard <- App_Scorecard(final_Model_For_Scorecard,test_data)
rejecteddata_scorecard <- App_Scorecard(final_Model_For_Scorecard,rejected_records)
#Optimal Cutoff =  0.049  - for the unbalanced model
points0 = 400
odds0 = 10
pdo = 20
factor = pdo / log(2)
offset = points0 - factor * log( odds0 )
cutoff_prob_from_model = .049
cutoff_logodd <- log((1-cutoff_prob_from_model)/cutoff_prob_from_model)
cutoff_score <- offset + factor * cutoff_logodd
cutoff_score
#419.33
## rejected data analysis
nrow(rejecteddata_scorecard[(rejecteddata_scorecard$Score >= cutoff_score),])
#55
boxplot(rejecteddata_scorecard$Score)
##  With this build we would have got 55 good customers who had been rejected.
## Full data analysis
fulldata <- customer_master_data
fulldata_scorecard <- App_Scorecard(final_Model_For_Scorecard,fulldata)
ggplot(fulldata_scorecard,aes(fulldata_scorecard$Score,fill=fulldata_scorecard$Performance))+ geom_histogram(binwidth = 10,colour="black")
fulldata_scorecard$predict_performance <- ifelse(fulldata_scorecard$bad>=0.049,1,0)
fulldata_scorecard$iswrong <- ifelse(fulldata_scorecard$predict_performance != fulldata_scorecard$Performance,1,0)
percent_of_wrongprediction <- (sum(fulldata_scorecard$iswrong)/nrow(fulldata_scorecard)) * 100
percent_of_wrongprediction
##-------------------  expected credit loss-------------------------S
#Expected loss(c1) = PD * EAD * LGD
#PD = Probability of default of each customer
#EAD = Exposure at default or oustanding
#LGD = Loss given default.
#Lets assume if recovery likelihood is 30% then LDG = 1  - 0.30 = 0.7
#Total loss expected if all customers are bad
fulldata_scorecard$expected_loss = fulldata_scorecard$bad * fulldata_scorecard$Outstanding.Balance_imputed * 0.7
# Calculated on Full data
# Total prospect loss =  2634047450
# (Prob of bad * Exposure at default * Loss given default)
# Expected loss by default customer from model 147718048
#
# The loss amount of 147718048 can be straight away avoided by not giving loan to default customer prospects
# However, by looking into the application score card, some customers of default category can be consider at medium risk because they fall in the boundary range.
# This potential credit loss can be minimized by target those customer, which Credit Score falls within Good and Intermediate.
# The verification / acquisition cost of Bad Customer can be minimized by this Model
#Creating a dataframe for loss calculation
potential_credit_loss <- fulldata_scorecard[, c("Performance", "bad","Outstanding.Balance_imputed")]
#Subsetting for the defaulted customers
loss_default_customer <- potential_credit_loss[(potential_credit_loss$Performance == 1),]
#Loss if the model is being used on the defaulted customer
loss_default_customer$loss_model <- as.integer(loss_default_customer$bad * loss_default_customer$Outstanding.Balance_imputed * 0.7)
#Calculating the total expected loss and the loss with the model.
total_expected_loss = sum(fulldata_scorecard$expected_loss)
total_extected_loss_default_cust <- sum(loss_default_customer$loss_model)
print(total_expected_loss)
print(total_extected_loss_default_cust)
#auto rejection rate
auto_rejection_rate <- sum(fulldata_scorecard$predict_performance)/nrow(fulldata_scorecard)
auto_approval_rate = 1 - auto_rejection_rate
auto_approval_rate
# Auto approval rate is 62.64%
# Rejected data analysis
#Number of good customers that is being rejected
nrow(rejecteddata_scorecard[(rejecteddata_scorecard$Score >= cutoff_score),])
#55
boxplot(rejecteddata_scorecard$Score)
# The histograms plots indicates that the number of defaulters decreases after Cut-off Score of 419
# Even though 419 is boundary value with Good and Bad Customers, we can suggest that the boundary range of customers fall between Good and Bad.
rejecteddata_scorecard$expected_loss = rejecteddata_scorecard$bad * rejecteddata_scorecard$Outstanding.Balance_imputed * 0.7
rejecteddata_loss <- rejecteddata_scorecard[,c("Score","bad","Outstanding.Balance_imputed")]
rejecteddata_cutoff_score <- rejecteddata_loss[(rejecteddata_loss$Score >= 419),]
loss_by_rejected_good_customer <- sum(rejecteddata_cutoff_score$Outstanding.Balance_imputed * 0.7)
# Total prospect loss =  96026810
# (Loss because of the full rejected data)
#
# Loss due of Rejection of Good customers is 43876837
#
# The amount of 43876837 would have been gained on using the model because it was the loss by rejection the good customers
rejecteddata_expected_loss <- sum(rejecteddata_scorecard$expected_loss)
print(rejecteddata_expected_loss)
# 96026810
print(loss_by_rejected_good_customer)
# 43876837
